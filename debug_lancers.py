#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import aiohttp
import json
import re
import os
import sys
import traceback
from datetime import datetime, timedelta
from playwright.async_api import async_playwright

# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
DEBUG = True

def debug_print(message):
    """ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›"""
    if DEBUG:
        print(f"[DEBUG] {message}")

# è¨­å®š
LANCERS_SEARCH_URL = "https://www.lancers.jp/work/search/system?budget_from=&budget_to=&work_rank%5B%5D=&work_rank%5B%5D=&work_rank%5B%5D=&keyword=&sort=work_post_date"
MAX_JOBS_TO_FETCH = 20  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«å°‘ãªãã™ã‚‹
HEADLESS_MODE = False  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ–ãƒ©ã‚¦ã‚¶ã‚’è¡¨ç¤º

# å¼Šç¤¾ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆï¼ˆæ¤œç´¢ãƒ¯ãƒ¼ãƒ‰å„ªå…ˆç‰ˆï¼‰
COMPANY_SKILLS = {
    "è¶…é«˜å„ªå…ˆåº¦": ["AI", "GPT", "ChatGPT", "Python", "API", "Django", "Next.js","React","TypeScript", "æ©Ÿæ¢°å­¦ç¿’"],
    "é«˜å„ªå…ˆåº¦": ["bot", "Talend", "Java", "ã‚¹ãƒãƒ›ã‚¢ãƒ—ãƒª", "ãƒ¢ãƒã‚¤ãƒ«é–‹ç™º", "äººå·¥çŸ¥èƒ½"],
    "ä¸­å„ªå…ˆåº¦": ["åŠ¹ç‡åŒ–", "ãƒ„ãƒ¼ãƒ«", "é–‹ç™º", "ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º", "React", "Node.js", "è‡ªå‹•åŒ–", "ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"],
    "ä½å„ªå…ˆåº¦": ["PostgreSQL", "MySQL", "ç¤¾å†…ãƒ„ãƒ¼ãƒ«", "æ¥­å‹™æ”¹å–„", "ã‚¢ãƒ—ãƒª", "ã‚µã‚¤ãƒˆ", "ç®¡ç†"],
    "æœ€ä½å„ªå…ˆåº¦": ["Render", "ãƒ­ãƒªãƒƒãƒãƒƒãƒ—", "WordPress", "PHP"]
}

# é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
EXCLUDE_KEYWORDS = [
    "æ±‚äºº", "æ¡ç”¨", "è»¢è·", "æ­£ç¤¾å“¡", "ã‚¢ãƒ«ãƒã‚¤ãƒˆ", "æ´¾é£",
    "ã‚³ãƒ³ãƒš", "ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³", "ã‚³ãƒ³ãƒ†ã‚¹ãƒˆ",
    "å‹Ÿé›†çµ‚äº†", "çµ‚äº†", "ç· åˆ‡", "CAD",
]

class CompleteJobsNotifier:
    def __init__(self):
        self.jobs_data = []
        self.seen_links = set()
        debug_print("CompleteJobsNotifieråˆæœŸåŒ–å®Œäº†")
        
    async def test_basic_connection(self):
        """åŸºæœ¬çš„ãªæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        debug_print("åŸºæœ¬çš„ãªæ¥ç¶šãƒ†ã‚¹ãƒˆã‚’é–‹å§‹...")
        
        try:
            async with async_playwright() as p:
                debug_print("Playwrightèµ·å‹•ä¸­...")
                browser = await p.chromium.launch(headless=HEADLESS_MODE, slow_mo=500)
                debug_print("ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•å®Œäº†")
                
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    locale="ja-JP"
                )
                page = await context.new_page()
                debug_print("æ–°ã—ã„ãƒšãƒ¼ã‚¸ä½œæˆå®Œäº†")
                
                debug_print(f"ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {LANCERS_SEARCH_URL}")
                response = await page.goto(LANCERS_SEARCH_URL, wait_until="domcontentloaded", timeout=30000)
                debug_print(f"ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº† (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status})")
                
                await page.wait_for_timeout(3000)
                
                # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                title = await page.title()
                debug_print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title}")
                
                # åŸºæœ¬çš„ãªè¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
                job_links = await page.query_selector_all("a[href*='/work/detail/']")
                debug_print(f"æ¡ˆä»¶ãƒªãƒ³ã‚¯æ•°: {len(job_links)}")
                
                if len(job_links) > 0:
                    debug_print("æ¡ˆä»¶ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼")
                    for i, link in enumerate(job_links[:3]):  # æœ€åˆã®3ã¤ã ã‘ãƒã‚§ãƒƒã‚¯
                        href = await link.get_attribute("href")
                        text = await link.text_content()
                        debug_print(f"ãƒªãƒ³ã‚¯ {i+1}: {text[:50]}... -> {href}")
                else:
                    debug_print("âŒ æ¡ˆä»¶ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    
                    # ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ç¢ºèª
                    content = await page.content()
                    debug_print(f"ãƒšãƒ¼ã‚¸å†…å®¹ã®é•·ã•: {len(content)}")
                    
                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    error_elements = await page.query_selector_all(".error, .alert, .warning")
                    for elem in error_elements:
                        error_text = await elem.text_content()
                        debug_print(f"ã‚¨ãƒ©ãƒ¼è¦ç´ : {error_text}")
                
                await browser.close()
                debug_print("ãƒ–ãƒ©ã‚¦ã‚¶ã‚¯ãƒ­ãƒ¼ã‚ºå®Œäº†")
                
        except Exception as e:
            debug_print(f"âŒ æ¥ç¶šãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
        
    async def fetch_jobs(self):
        """å…¨æ¡ˆä»¶ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰"""
        debug_print("ğŸš€ Lancersæ¡ˆä»¶å–å¾—ã‚’é–‹å§‹...")
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=HEADLESS_MODE, slow_mo=300)
                debug_print("ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•å®Œäº†")
                
                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    locale="ja-JP"
                )
                page = await context.new_page()
                
                debug_print(f"ğŸ“¡ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {LANCERS_SEARCH_URL}")
                response = await page.goto(LANCERS_SEARCH_URL, wait_until="domcontentloaded", timeout=60000)
                debug_print(f"âœ… ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº† (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status})")
                
                await page.wait_for_timeout(5000)
                
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦èª­ã¿è¾¼ã¿
                debug_print("ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­...")
                await self.scroll_and_load_more(page)
                
                # æ¡ˆä»¶ãƒªãƒ³ã‚¯ã‚’å–å¾—
                job_elements = await page.query_selector_all("a[href*='/work/detail/']")
                debug_print(f"ğŸ“Š {len(job_elements)} å€‹ã®æ¡ˆä»¶å€™è£œã‚’ç™ºè¦‹")
                
                if len(job_elements) == 0:
                    debug_print("âŒ æ¡ˆä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’ç¢ºèªã—ã¾ã™...")
                    
                    # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
                    alternative_selectors = [
                        "a[href*='work']",
                        ".job-item a",
                        ".work-item a",
                        "a[href*='detail']"
                    ]
                    
                    for selector in alternative_selectors:
                        alt_elements = await page.query_selector_all(selector)
                        debug_print(f"ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}': {len(alt_elements)}å€‹")
                
                all_jobs = []
                
                for i, element in enumerate(job_elements):
                    if len(all_jobs) >= MAX_JOBS_TO_FETCH:
                        break
                        
                    debug_print(f"æ¡ˆä»¶ {i+1}/{len(job_elements)} ã‚’å‡¦ç†ä¸­...")
                    job_info = await self.extract_job_info(element, page)
                    
                    if job_info:
                        debug_print(f"æ¡ˆä»¶æƒ…å ±å–å¾—æˆåŠŸ: {job_info['title'][:30]}...")
                        if self.should_include_job_minimal(job_info):
                            all_jobs.append(job_info)
                            debug_print(f"âœ… æ¡ˆä»¶è¿½åŠ : {len(all_jobs)}ä»¶ç›®")
                        else:
                            debug_print("âŒ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šé™¤å¤–")
                    else:
                        debug_print("âŒ æ¡ˆä»¶æƒ…å ±å–å¾—å¤±æ•—")
                
                debug_print(f"âœ… åˆè¨ˆ {len(all_jobs)} ä»¶ã®æ¡ˆä»¶ã‚’å–å¾—ã—ã¾ã—ãŸ")
                
                await browser.close()
                return all_jobs
                
        except Exception as e:
            debug_print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            return []
    
    async def scroll_and_load_more(self, page):
        """ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«"""
        try:
            debug_print("ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹...")
            
            for i in range(3):  # å›æ•°ã‚’æ¸›ã‚‰ã™
                debug_print(f"ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« {i+1}/3")
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(2000)
            
            # ã‚‚ã£ã¨è¦‹ã‚‹ãƒœã‚¿ãƒ³ã‚’æ¢ã™
            more_button = await page.query_selector(".more-button, .load-more, [class*='more']")
            if more_button:
                debug_print("ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯")
                await more_button.click()
                await page.wait_for_timeout(3000)
            else:
                debug_print("ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            debug_print(f"âš ï¸ ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def extract_job_info(self, element, page):
        """æ¡ˆä»¶æƒ…å ±ã‚’æŠ½å‡ºï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰"""
        try:
            title_text = await element.text_content()
            href = await element.get_attribute("href")
            
            debug_print(f"ã‚¿ã‚¤ãƒˆãƒ«: {title_text[:50] if title_text else 'None'}...")
            debug_print(f"ãƒªãƒ³ã‚¯: {href}")
            
            if not title_text or not href:
                debug_print("âŒ ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯ãƒªãƒ³ã‚¯ãŒç©ºã§ã™")
                return None
            
            title = self.clean_title(title_text)
            if not title or len(title) < 5:
                debug_print("âŒ ã‚¿ã‚¤ãƒˆãƒ«ãŒçŸ­ã™ãã¾ã™")
                return None
            
            if href.startswith("/"):
                href = "https://www.lancers.jp" + href
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if href in self.seen_links:
                debug_print("âŒ é‡è¤‡ãƒªãƒ³ã‚¯ã§ã™")
                return None
            self.seen_links.add(href)
            
            # åŸºæœ¬çš„ãªæ¡ˆä»¶æƒ…å ±ã‚’ä½œæˆ
            job_info = {
                "title": title,
                "link": href,
                "price": "ä¾¡æ ¼æƒ…å ±ãªã—",
                "deadline": "æœŸé™æƒ…å ±ãªã—",
                "start_date": "é–‹å§‹æ—¥æƒ…å ±ãªã—",
                "delivery_date": "ç´æœŸæƒ…å ±ãªã—",
                "applicant_count": "0",
                "recruitment_count": "1",
                "client_name": "ä¾é ¼è€…æƒ…å ±ãªã—",
                "status": "å‹Ÿé›†ä¸­",
                "urgency": False,
                "category": "ã‚·ã‚¹ãƒ†ãƒ é–‹ç™º",
                "corporate_allowed": False,
                "skill_matches": [],
                "skill_count": 0,
                "priority_score": 1,
                "scraped_at": datetime.now().isoformat()
            }
            
            debug_print(f"âœ… åŸºæœ¬æƒ…å ±ä½œæˆå®Œäº†: {title[:30]}...")
            return job_info
            
        except Exception as e:
            debug_print(f"âš ï¸ æ¡ˆä»¶æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def clean_title(self, title):
        """ã‚¿ã‚¤ãƒˆãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not title:
            return ""
        
        title = re.sub(r'\s+', ' ', title.strip())
        title = re.sub(r'^(NEW\s*){1,}', '', title, flags=re.IGNORECASE)
        
        return title.strip()
    
    def should_include_job_minimal(self, job_info):
        """åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        title = job_info["title"]
        
        if not title or len(title.strip()) < 5:
            return False
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        title_lower = title.lower()
        for keyword in EXCLUDE_KEYWORDS:
            if keyword.lower() in title_lower:
                debug_print(f"é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿: {keyword}")
                return False
        
        return True
    
    def save_debug_data(self, jobs):
        """ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        timestamp = datetime.now()
        
        data = {
            "timestamp": timestamp.isoformat(),
            "count": len(jobs),
            "type": "ãƒ‡ãƒãƒƒã‚°ç”¨å…¨æ¡ˆä»¶ãƒªã‚¹ãƒˆ",
            "jobs": jobs
        }
        
        filename = f"debug_jobs_{timestamp.strftime('%Y%m%d_%H%M')}.json"
        
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            debug_print(f"ğŸ’¾ ãƒ‡ãƒãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {filename}")
        except Exception as e:
            debug_print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰"""
    print("=" * 70)
    print("ğŸ¤– Lancersæ¡ˆä»¶å–å¾—ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    print("=" * 70)
    
    try:
        notifier = CompleteJobsNotifier()
        
        # ã¾ãšåŸºæœ¬çš„ãªæ¥ç¶šãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        debug_print("=== åŸºæœ¬æ¥ç¶šãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        await notifier.test_basic_connection()
        
        # å®Ÿéš›ã®æ¡ˆä»¶å–å¾—ã‚’å®Ÿè¡Œ
        debug_print("=== æ¡ˆä»¶å–å¾—é–‹å§‹ ===")
        jobs = await notifier.fetch_jobs()
        
        if jobs:
            debug_print(f"âœ… {len(jobs)}ä»¶ã®æ¡ˆä»¶ã‚’å–å¾—ã—ã¾ã—ãŸ")
            notifier.save_debug_data(jobs)
            
            print("\n" + "=" * 70)
            print("ğŸ“Š å®Ÿè¡Œçµæœ:")
            print("=" * 70)
            print(f"âœ… å–å¾—æ¡ˆä»¶æ•°: {len(jobs)}ä»¶")
            
            # æœ€åˆã®3ä»¶ã‚’è¡¨ç¤º
            for i, job in enumerate(jobs[:3], 1):
                print(f"\n{i}. {job['title']}")
                print(f"   ãƒªãƒ³ã‚¯: {job['link']}")
                
        else:
            debug_print("âŒ æ¡ˆä»¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            print("âŒ æ¡ˆä»¶å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            
    except Exception as e:
        debug_print(f"âŒ ãƒ¡ã‚¤ãƒ³é–¢æ•°ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    # Pythonã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    debug_print(f"Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    debug_print(f"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
    
    asyncio.run(main())